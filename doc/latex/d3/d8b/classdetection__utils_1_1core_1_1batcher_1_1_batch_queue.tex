\doxysection{detection\+\_\+utils.\+core.\+batcher.\+Batch\+Queue Class Reference}
\hypertarget{classdetection__utils_1_1core_1_1batcher_1_1_batch_queue}{}\label{classdetection__utils_1_1core_1_1batcher_1_1_batch_queue}\index{detection\_utils.core.batcher.BatchQueue@{detection\_utils.core.batcher.BatchQueue}}


Inheritance diagram for detection\+\_\+utils.\+core.\+batcher.\+Batch\+Queue\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=214pt]{dc/dc0/classdetection__utils_1_1core_1_1batcher_1_1_batch_queue__inherit__graph}
\end{center}
\end{figure}


Collaboration diagram for detection\+\_\+utils.\+core.\+batcher.\+Batch\+Queue\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=214pt]{dd/dc7/classdetection__utils_1_1core_1_1batcher_1_1_batch_queue__coll__graph}
\end{center}
\end{figure}
\doxysubsubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classdetection__utils_1_1core_1_1batcher_1_1_batch_queue_a94f3672abbe06444c2ce242e6725b535}{\+\_\+\+\_\+init\+\_\+\+\_\+}} (self, tensor\+\_\+dict, batch\+\_\+size, batch\+\_\+queue\+\_\+capacity, num\+\_\+batch\+\_\+queue\+\_\+threads, prefetch\+\_\+queue\+\_\+capacity)
\item 
\mbox{\hyperlink{classdetection__utils_1_1core_1_1batcher_1_1_batch_queue_a507d2f6936edf4bcde5c5b79cf2d1ade}{dequeue}} (self)
\end{DoxyCompactItemize}
\doxysubsubsection*{Protected Attributes}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classdetection__utils_1_1core_1_1batcher_1_1_batch_queue_a6c1f9dc63a735db936d584e93cb06c28}{\+\_\+queue}}
\item 
\mbox{\hyperlink{classdetection__utils_1_1core_1_1batcher_1_1_batch_queue_a09849dc49848ec57acf39bd7f2485547}{\+\_\+static\+\_\+shapes}}
\item 
\mbox{\hyperlink{classdetection__utils_1_1core_1_1batcher_1_1_batch_queue_aae0bf8fad6b18e347fe840d2d8ea2fa9}{\+\_\+batch\+\_\+size}}
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}BatchQueue class.

This class creates a batch queue to asynchronously enqueue tensors_dict.
It also adds a FIFO prefetcher so that the batches are readily available
for the consumers.  Dequeue ops for a BatchQueue object can be created via
the Dequeue method which evaluates to a batch of tensor_dict.

Example input pipeline with batching:
------------------------------------
key, string_tensor = slim.parallel_reader.parallel_read(...)
tensor_dict = decoder.decode(string_tensor)
tensor_dict = preprocessor.preprocess(tensor_dict, ...)
batch_queue = batcher.BatchQueue(tensor_dict,
                                 batch_size=32,
                                 batch_queue_capacity=2000,
                                 num_batch_queue_threads=8,
                                 prefetch_queue_capacity=20)
tensor_dict = batch_queue.dequeue()
outputs = Model(tensor_dict)
...
-----------------------------------

Notes:
-----
This class batches tensors of unequal sizes by zero padding and unpadding
them after generating a batch. This can be computationally expensive when
batching tensors (such as images) that are of vastly different sizes. So it is
recommended that the shapes of such tensors be fully defined in tensor_dict
while other lightweight tensors such as bounding box corners and class labels
can be of varying sizes. Use either crop or resize operations to fully define
the shape of an image in tensor_dict.

It is also recommended to perform any preprocessing operations on tensors
before passing to BatchQueue and subsequently calling the Dequeue method.

Another caveat is that this class does not read the last batch if it is not
full. The current implementation makes it hard to support that use case. So,
for evaluation, when it is critical to run all the examples through your
network use the input pipeline example mentioned in core/prefetcher.py.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{batcher_8py_source_l00031}{31}} of file \mbox{\hyperlink{batcher_8py_source}{batcher.\+py}}.



\doxysubsection{Constructor \& Destructor Documentation}
\Hypertarget{classdetection__utils_1_1core_1_1batcher_1_1_batch_queue_a94f3672abbe06444c2ce242e6725b535}\label{classdetection__utils_1_1core_1_1batcher_1_1_batch_queue_a94f3672abbe06444c2ce242e6725b535} 
\index{detection\_utils.core.batcher.BatchQueue@{detection\_utils.core.batcher.BatchQueue}!\_\_init\_\_@{\_\_init\_\_}}
\index{\_\_init\_\_@{\_\_init\_\_}!detection\_utils.core.batcher.BatchQueue@{detection\_utils.core.batcher.BatchQueue}}
\doxysubsubsection{\texorpdfstring{\_\_init\_\_()}{\_\_init\_\_()}}
{\footnotesize\ttfamily detection\+\_\+utils.\+core.\+batcher.\+Batch\+Queue.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{tensor\+\_\+dict,  }\item[{}]{batch\+\_\+size,  }\item[{}]{batch\+\_\+queue\+\_\+capacity,  }\item[{}]{num\+\_\+batch\+\_\+queue\+\_\+threads,  }\item[{}]{prefetch\+\_\+queue\+\_\+capacity }\end{DoxyParamCaption})}

\begin{DoxyVerb}Constructs a batch queue holding tensor_dict.

Args:
  tensor_dict: dictionary of tensors to batch.
  batch_size: batch size.
  batch_queue_capacity: max capacity of the queue from which the tensors are
    batched.
  num_batch_queue_threads: number of threads to use for batching.
  prefetch_queue_capacity: max capacity of the queue used to prefetch
    assembled batches.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{batcher_8py_source_l00073}{73}} of file \mbox{\hyperlink{batcher_8py_source}{batcher.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00074\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ num\_batch\_queue\_threads,\ prefetch\_queue\_capacity):}
\DoxyCodeLine{00075\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Constructs\ a\ batch\ queue\ holding\ tensor\_dict.}}
\DoxyCodeLine{00076\ \textcolor{stringliteral}{}}
\DoxyCodeLine{00077\ \textcolor{stringliteral}{\ \ \ \ Args:}}
\DoxyCodeLine{00078\ \textcolor{stringliteral}{\ \ \ \ \ \ tensor\_dict:\ dictionary\ of\ tensors\ to\ batch.}}
\DoxyCodeLine{00079\ \textcolor{stringliteral}{\ \ \ \ \ \ batch\_size:\ batch\ size.}}
\DoxyCodeLine{00080\ \textcolor{stringliteral}{\ \ \ \ \ \ batch\_queue\_capacity:\ max\ capacity\ of\ the\ queue\ }\textcolor{keyword}{from}\ which\ the\ tensors\ are}
\DoxyCodeLine{00081\ \ \ \ \ \ \ \ \ batched.}
\DoxyCodeLine{00082\ \ \ \ \ \ \ num\_batch\_queue\_threads:\ number\ of\ threads\ to\ use\ \textcolor{keywordflow}{for}\ batching.}
\DoxyCodeLine{00083\ \ \ \ \ \ \ prefetch\_queue\_capacity:\ max\ capacity\ of\ the\ queue\ used\ to\ prefetch}
\DoxyCodeLine{00084\ \ \ \ \ \ \ \ \ assembled\ batches.}
\DoxyCodeLine{00085\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}}}
\DoxyCodeLine{00086\ \textcolor{stringliteral}{\ \ \ \ }\textcolor{comment}{\#\ Remember\ static\ shapes\ to\ set\ shapes\ of\ batched\ tensors.}}
\DoxyCodeLine{00087\ \ \ \ \ static\_shapes\ =\ collections.OrderedDict(}
\DoxyCodeLine{00088\ \ \ \ \ \ \ \ \ \{key:\ tensor.get\_shape()\ \textcolor{keywordflow}{for}\ key,\ tensor\ \textcolor{keywordflow}{in}\ tensor\_dict.items()\})}
\DoxyCodeLine{00089\ \ \ \ \ \textcolor{comment}{\#\ Remember\ runtime\ shapes\ to\ unpad\ tensors\ after\ batching.}}
\DoxyCodeLine{00090\ \ \ \ \ runtime\_shapes\ =\ collections.OrderedDict(}
\DoxyCodeLine{00091\ \ \ \ \ \ \ \ \ \{(key\ +\ rt\_shape\_str):\ tf.shape(tensor)}
\DoxyCodeLine{00092\ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ key,\ tensor\ \textcolor{keywordflow}{in}\ tensor\_dict.items()\})}
\DoxyCodeLine{00093\ }
\DoxyCodeLine{00094\ \ \ \ \ all\_tensors\ =\ tensor\_dict}
\DoxyCodeLine{00095\ \ \ \ \ all\_tensors.update(runtime\_shapes)}
\DoxyCodeLine{00096\ \ \ \ \ batched\_tensors\ =\ tf.train.batch(}
\DoxyCodeLine{00097\ \ \ \ \ \ \ \ \ all\_tensors,}
\DoxyCodeLine{00098\ \ \ \ \ \ \ \ \ capacity=batch\_queue\_capacity,}
\DoxyCodeLine{00099\ \ \ \ \ \ \ \ \ batch\_size=batch\_size,}
\DoxyCodeLine{00100\ \ \ \ \ \ \ \ \ dynamic\_pad=\textcolor{keyword}{True},}
\DoxyCodeLine{00101\ \ \ \ \ \ \ \ \ num\_threads=num\_batch\_queue\_threads)}
\DoxyCodeLine{00102\ }
\DoxyCodeLine{00103\ \ \ \ \ self.\_queue\ =\ prefetcher.prefetch(batched\_tensors,}
\DoxyCodeLine{00104\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ prefetch\_queue\_capacity)}
\DoxyCodeLine{00105\ \ \ \ \ self.\_static\_shapes\ =\ static\_shapes}
\DoxyCodeLine{00106\ \ \ \ \ self.\_batch\_size\ =\ batch\_size}
\DoxyCodeLine{00107\ }

\end{DoxyCode}


\doxysubsection{Member Function Documentation}
\Hypertarget{classdetection__utils_1_1core_1_1batcher_1_1_batch_queue_a507d2f6936edf4bcde5c5b79cf2d1ade}\label{classdetection__utils_1_1core_1_1batcher_1_1_batch_queue_a507d2f6936edf4bcde5c5b79cf2d1ade} 
\index{detection\_utils.core.batcher.BatchQueue@{detection\_utils.core.batcher.BatchQueue}!dequeue@{dequeue}}
\index{dequeue@{dequeue}!detection\_utils.core.batcher.BatchQueue@{detection\_utils.core.batcher.BatchQueue}}
\doxysubsubsection{\texorpdfstring{dequeue()}{dequeue()}}
{\footnotesize\ttfamily detection\+\_\+utils.\+core.\+batcher.\+Batch\+Queue.\+dequeue (\begin{DoxyParamCaption}\item[{}]{self }\end{DoxyParamCaption})}

\begin{DoxyVerb}Dequeues a batch of tensor_dict from the BatchQueue.

TODO: use allow_smaller_final_batch to allow running over the whole eval set

Returns:
  A list of tensor_dicts of the requested batch_size.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{batcher_8py_source_l00108}{108}} of file \mbox{\hyperlink{batcher_8py_source}{batcher.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00108\ \ \ \textcolor{keyword}{def\ }dequeue(self):}
\DoxyCodeLine{00109\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Dequeues\ a\ batch\ of\ tensor\_dict\ from\ the\ BatchQueue.}}
\DoxyCodeLine{00110\ \textcolor{stringliteral}{}}
\DoxyCodeLine{00111\ \textcolor{stringliteral}{\ \ \ \ TODO:\ use\ allow\_smaller\_final\_batch\ to\ allow\ running\ over\ the\ whole\ eval\ set}}
\DoxyCodeLine{00112\ \textcolor{stringliteral}{}}
\DoxyCodeLine{00113\ \textcolor{stringliteral}{\ \ \ \ Returns:}}
\DoxyCodeLine{00114\ \textcolor{stringliteral}{\ \ \ \ \ \ A\ list\ of\ tensor\_dicts\ of\ the\ requested\ batch\_size.}}
\DoxyCodeLine{00115\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{00116\ \textcolor{stringliteral}{\ \ \ \ batched\_tensors\ =\ self.\_queue.dequeue()}}
\DoxyCodeLine{00117\ \textcolor{stringliteral}{\ \ \ \ }\textcolor{comment}{\#\ Separate\ input\ tensors\ from\ tensors\ containing\ their\ runtime\ shapes.}}
\DoxyCodeLine{00118\ \ \ \ \ tensors\ =\ \{\}}
\DoxyCodeLine{00119\ \ \ \ \ shapes\ =\ \{\}}
\DoxyCodeLine{00120\ \ \ \ \ \textcolor{keywordflow}{for}\ key,\ batched\_tensor\ \textcolor{keywordflow}{in}\ batched\_tensors.items():}
\DoxyCodeLine{00121\ \ \ \ \ \ \ unbatched\_tensor\_list\ =\ tf.unstack(batched\_tensor)}
\DoxyCodeLine{00122\ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ i,\ unbatched\_tensor\ \textcolor{keywordflow}{in}\ enumerate(unbatched\_tensor\_list):}
\DoxyCodeLine{00123\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ rt\_shape\_str\ \textcolor{keywordflow}{in}\ key:}
\DoxyCodeLine{00124\ \ \ \ \ \ \ \ \ \ \ shapes[(key[:-\/len(rt\_shape\_str)],\ i)]\ =\ unbatched\_tensor}
\DoxyCodeLine{00125\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{00126\ \ \ \ \ \ \ \ \ \ \ tensors[(key,\ i)]\ =\ unbatched\_tensor}
\DoxyCodeLine{00127\ }
\DoxyCodeLine{00128\ \ \ \ \ \textcolor{comment}{\#\ Undo\ that\ padding\ using\ shapes\ and\ create\ a\ list\ of\ size\ \`{}batch\_size`\ that}}
\DoxyCodeLine{00129\ \ \ \ \ \textcolor{comment}{\#\ contains\ tensor\ dictionaries.}}
\DoxyCodeLine{00130\ \ \ \ \ tensor\_dict\_list\ =\ []}
\DoxyCodeLine{00131\ \ \ \ \ batch\_size\ =\ self.\_batch\_size}
\DoxyCodeLine{00132\ \ \ \ \ \textcolor{keywordflow}{for}\ batch\_id\ \textcolor{keywordflow}{in}\ range(batch\_size):}
\DoxyCodeLine{00133\ \ \ \ \ \ \ tensor\_dict\ =\ \{\}}
\DoxyCodeLine{00134\ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ key\ \textcolor{keywordflow}{in}\ self.\_static\_shapes:}
\DoxyCodeLine{00135\ \ \ \ \ \ \ \ \ tensor\_dict[key]\ =\ tf.slice(tensors[(key,\ batch\_id)],}
\DoxyCodeLine{00136\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ tf.zeros\_like(shapes[(key,\ batch\_id)]),}
\DoxyCodeLine{00137\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ shapes[(key,\ batch\_id)])}
\DoxyCodeLine{00138\ \ \ \ \ \ \ \ \ tensor\_dict[key].set\_shape(self.\_static\_shapes[key])}
\DoxyCodeLine{00139\ \ \ \ \ \ \ tensor\_dict\_list.append(tensor\_dict)}
\DoxyCodeLine{00140\ }
\DoxyCodeLine{00141\ \ \ \ \ \textcolor{keywordflow}{return}\ tensor\_dict\_list}

\end{DoxyCode}
Here is the call graph for this function\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=220pt]{d3/d8b/classdetection__utils_1_1core_1_1batcher_1_1_batch_queue_a507d2f6936edf4bcde5c5b79cf2d1ade_cgraph}
\end{center}
\end{figure}
Here is the caller graph for this function\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=220pt]{d3/d8b/classdetection__utils_1_1core_1_1batcher_1_1_batch_queue_a507d2f6936edf4bcde5c5b79cf2d1ade_icgraph}
\end{center}
\end{figure}


\doxysubsection{Member Data Documentation}
\Hypertarget{classdetection__utils_1_1core_1_1batcher_1_1_batch_queue_aae0bf8fad6b18e347fe840d2d8ea2fa9}\label{classdetection__utils_1_1core_1_1batcher_1_1_batch_queue_aae0bf8fad6b18e347fe840d2d8ea2fa9} 
\index{detection\_utils.core.batcher.BatchQueue@{detection\_utils.core.batcher.BatchQueue}!\_batch\_size@{\_batch\_size}}
\index{\_batch\_size@{\_batch\_size}!detection\_utils.core.batcher.BatchQueue@{detection\_utils.core.batcher.BatchQueue}}
\doxysubsubsection{\texorpdfstring{\_batch\_size}{\_batch\_size}}
{\footnotesize\ttfamily detection\+\_\+utils.\+core.\+batcher.\+Batch\+Queue.\+\_\+batch\+\_\+size\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{batcher_8py_source_l00106}{106}} of file \mbox{\hyperlink{batcher_8py_source}{batcher.\+py}}.

\Hypertarget{classdetection__utils_1_1core_1_1batcher_1_1_batch_queue_a6c1f9dc63a735db936d584e93cb06c28}\label{classdetection__utils_1_1core_1_1batcher_1_1_batch_queue_a6c1f9dc63a735db936d584e93cb06c28} 
\index{detection\_utils.core.batcher.BatchQueue@{detection\_utils.core.batcher.BatchQueue}!\_queue@{\_queue}}
\index{\_queue@{\_queue}!detection\_utils.core.batcher.BatchQueue@{detection\_utils.core.batcher.BatchQueue}}
\doxysubsubsection{\texorpdfstring{\_queue}{\_queue}}
{\footnotesize\ttfamily detection\+\_\+utils.\+core.\+batcher.\+Batch\+Queue.\+\_\+queue\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{batcher_8py_source_l00103}{103}} of file \mbox{\hyperlink{batcher_8py_source}{batcher.\+py}}.

\Hypertarget{classdetection__utils_1_1core_1_1batcher_1_1_batch_queue_a09849dc49848ec57acf39bd7f2485547}\label{classdetection__utils_1_1core_1_1batcher_1_1_batch_queue_a09849dc49848ec57acf39bd7f2485547} 
\index{detection\_utils.core.batcher.BatchQueue@{detection\_utils.core.batcher.BatchQueue}!\_static\_shapes@{\_static\_shapes}}
\index{\_static\_shapes@{\_static\_shapes}!detection\_utils.core.batcher.BatchQueue@{detection\_utils.core.batcher.BatchQueue}}
\doxysubsubsection{\texorpdfstring{\_static\_shapes}{\_static\_shapes}}
{\footnotesize\ttfamily detection\+\_\+utils.\+core.\+batcher.\+Batch\+Queue.\+\_\+static\+\_\+shapes\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{batcher_8py_source_l00105}{105}} of file \mbox{\hyperlink{batcher_8py_source}{batcher.\+py}}.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
src/main/resources/processing/video/detections/detection\+\_\+utils/core/\mbox{\hyperlink{batcher_8py}{batcher.\+py}}\end{DoxyCompactItemize}
