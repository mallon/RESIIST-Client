\doxysection{detection\+\_\+utils.\+core.\+losses.\+Weighted\+Softmax\+Classification\+Against\+Logits\+Loss Class Reference}
\hypertarget{classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss}{}\label{classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss}\index{detection\_utils.core.losses.WeightedSoftmaxClassificationAgainstLogitsLoss@{detection\_utils.core.losses.WeightedSoftmaxClassificationAgainstLogitsLoss}}


Inheritance diagram for detection\+\_\+utils.\+core.\+losses.\+Weighted\+Softmax\+Classification\+Against\+Logits\+Loss\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=322pt]{d5/d75/classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss__inherit__graph}
\end{center}
\end{figure}


Collaboration diagram for detection\+\_\+utils.\+core.\+losses.\+Weighted\+Softmax\+Classification\+Against\+Logits\+Loss\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=322pt]{d6/d34/classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss__coll__graph}
\end{center}
\end{figure}
\doxysubsubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss_a8b44b227d04e7dd1b8e6079342a535fa}{\+\_\+\+\_\+init\+\_\+\+\_\+}} (self, logit\+\_\+scale=1.\+0)
\end{DoxyCompactItemize}
\doxysubsection*{Public Member Functions inherited from \mbox{\hyperlink{classdetection__utils_1_1core_1_1losses_1_1_loss}{detection\+\_\+utils.\+core.\+losses.\+Loss}}}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classdetection__utils_1_1core_1_1losses_1_1_loss_a8555bca99fcc03e1fd78e25e3b2e8843}{\+\_\+\+\_\+call\+\_\+\+\_\+}} (self, prediction\+\_\+tensor, target\+\_\+tensor, ignore\+\_\+nan\+\_\+targets=False, losses\+\_\+mask=None, scope=None, \texorpdfstring{$\ast$}{*}\texorpdfstring{$\ast$}{*}params)
\end{DoxyCompactItemize}
\doxysubsubsection*{Protected Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss_a7fb6902c62d6a331ae8dd5521b6a1089}{\+\_\+scale\+\_\+and\+\_\+softmax\+\_\+logits}} (self, logits)
\item 
\mbox{\hyperlink{classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss_a34c03e1dffb28a11c360679afedec8e4}{\+\_\+compute\+\_\+loss}} (self, prediction\+\_\+tensor, target\+\_\+tensor, weights)
\end{DoxyCompactItemize}
\doxysubsection*{Protected Member Functions inherited from \mbox{\hyperlink{classdetection__utils_1_1core_1_1losses_1_1_loss}{detection\+\_\+utils.\+core.\+losses.\+Loss}}}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classdetection__utils_1_1core_1_1losses_1_1_loss_aadcefe6d066fd31b3b5a2542d3fef144}{\+\_\+get\+\_\+loss\+\_\+multiplier\+\_\+for\+\_\+tensor}} (self, tensor, losses\+\_\+mask)
\item 
\mbox{\hyperlink{classdetection__utils_1_1core_1_1losses_1_1_loss_ae772289a8be37a802e11150c81e7ece7}{\+\_\+compute\+\_\+loss}} (self, prediction\+\_\+tensor, target\+\_\+tensor, \texorpdfstring{$\ast$}{*}\texorpdfstring{$\ast$}{*}params)
\end{DoxyCompactItemize}
\doxysubsubsection*{Protected Attributes}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss_a90b59234980cf95e5139fc5e2f00843e}{\+\_\+logit\+\_\+scale}}
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Softmax loss function against logits.

 Targets are expected to be provided in logits space instead of "one hot" or
 "probability distribution" space.
\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{losses_8py_source_l00354}{354}} of file \mbox{\hyperlink{losses_8py_source}{losses.\+py}}.



\doxysubsection{Constructor \& Destructor Documentation}
\Hypertarget{classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss_a8b44b227d04e7dd1b8e6079342a535fa}\label{classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss_a8b44b227d04e7dd1b8e6079342a535fa} 
\index{detection\_utils.core.losses.WeightedSoftmaxClassificationAgainstLogitsLoss@{detection\_utils.core.losses.WeightedSoftmaxClassificationAgainstLogitsLoss}!\_\_init\_\_@{\_\_init\_\_}}
\index{\_\_init\_\_@{\_\_init\_\_}!detection\_utils.core.losses.WeightedSoftmaxClassificationAgainstLogitsLoss@{detection\_utils.core.losses.WeightedSoftmaxClassificationAgainstLogitsLoss}}
\doxysubsubsection{\texorpdfstring{\_\_init\_\_()}{\_\_init\_\_()}}
{\footnotesize\ttfamily detection\+\_\+utils.\+core.\+losses.\+Weighted\+Softmax\+Classification\+Against\+Logits\+Loss.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{logit\+\_\+scale = {\ttfamily 1.0} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Constructor.

Args:
  logit_scale: When this value is high, the target is "diffused" and
               when this value is low, the target is made peakier.
               (default 1.0)\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{losses_8py_source_l00361}{361}} of file \mbox{\hyperlink{losses_8py_source}{losses.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00361\ \ \ \textcolor{keyword}{def\ }\_\_init\_\_(self,\ logit\_scale=1.0):}
\DoxyCodeLine{00362\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Constructor.}}
\DoxyCodeLine{00363\ \textcolor{stringliteral}{}}
\DoxyCodeLine{00364\ \textcolor{stringliteral}{\ \ \ \ Args:}}
\DoxyCodeLine{00365\ \textcolor{stringliteral}{\ \ \ \ \ \ logit\_scale:\ When\ this\ value\ }\textcolor{keywordflow}{is}\ high,\ the\ target\ \textcolor{keywordflow}{is}\ \textcolor{stringliteral}{"{}diffused"{}}\ \textcolor{keywordflow}{and}}
\DoxyCodeLine{00366\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ when\ this\ value\ \textcolor{keywordflow}{is}\ low,\ the\ target\ \textcolor{keywordflow}{is}\ made\ peakier.}
\DoxyCodeLine{00367\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (default\ 1.0)}
\DoxyCodeLine{00368\ }
\DoxyCodeLine{00369\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}}}
\DoxyCodeLine{00370\ \textcolor{stringliteral}{\ \ \ \ super(WeightedSoftmaxClassificationAgainstLogitsLoss,\ self).\_\_init\_\_()}}
\DoxyCodeLine{00371\ \textcolor{stringliteral}{\ \ \ \ self.\_logit\_scale\ =\ logit\_scale}}
\DoxyCodeLine{00372\ \textcolor{stringliteral}{}}

\end{DoxyCode}
Here is the call graph for this function\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=265pt]{d5/d4c/classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss_a8b44b227d04e7dd1b8e6079342a535fa_cgraph}
\end{center}
\end{figure}
Here is the caller graph for this function\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=265pt]{d5/d4c/classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss_a8b44b227d04e7dd1b8e6079342a535fa_icgraph}
\end{center}
\end{figure}


\doxysubsection{Member Function Documentation}
\Hypertarget{classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss_a34c03e1dffb28a11c360679afedec8e4}\label{classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss_a34c03e1dffb28a11c360679afedec8e4} 
\index{detection\_utils.core.losses.WeightedSoftmaxClassificationAgainstLogitsLoss@{detection\_utils.core.losses.WeightedSoftmaxClassificationAgainstLogitsLoss}!\_compute\_loss@{\_compute\_loss}}
\index{\_compute\_loss@{\_compute\_loss}!detection\_utils.core.losses.WeightedSoftmaxClassificationAgainstLogitsLoss@{detection\_utils.core.losses.WeightedSoftmaxClassificationAgainstLogitsLoss}}
\doxysubsubsection{\texorpdfstring{\_compute\_loss()}{\_compute\_loss()}}
{\footnotesize\ttfamily detection\+\_\+utils.\+core.\+losses.\+Weighted\+Softmax\+Classification\+Against\+Logits\+Loss.\+\_\+compute\+\_\+loss (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{prediction\+\_\+tensor,  }\item[{}]{target\+\_\+tensor,  }\item[{}]{weights }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Compute loss function.

Args:
  prediction_tensor: A float tensor of shape [batch_size, num_anchors,
    num_classes] representing the predicted logits for each class
  target_tensor: A float tensor of shape [batch_size, num_anchors,
    num_classes] representing logit classification targets
  weights: a float tensor of shape, either [batch_size, num_anchors,
    num_classes] or [batch_size, num_anchors, 1]. If the shape is
    [batch_size, num_anchors, 1], all the classses are equally weighted.

Returns:
  loss: a float tensor of shape [batch_size, num_anchors]
    representing the value of the loss function.
\end{DoxyVerb}
 

Reimplemented from \mbox{\hyperlink{classdetection__utils_1_1core_1_1losses_1_1_loss_ae772289a8be37a802e11150c81e7ece7}{detection\+\_\+utils.\+core.\+losses.\+Loss}}.



Definition at line \mbox{\hyperlink{losses_8py_source_l00378}{378}} of file \mbox{\hyperlink{losses_8py_source}{losses.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00378\ \ \ \textcolor{keyword}{def\ }\_compute\_loss(self,\ prediction\_tensor,\ target\_tensor,\ weights):}
\DoxyCodeLine{00379\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Compute\ loss\ function.}}
\DoxyCodeLine{00380\ \textcolor{stringliteral}{}}
\DoxyCodeLine{00381\ \textcolor{stringliteral}{\ \ \ \ Args:}}
\DoxyCodeLine{00382\ \textcolor{stringliteral}{\ \ \ \ \ \ prediction\_tensor:\ A\ float\ tensor\ of\ shape\ [batch\_size,\ num\_anchors,}}
\DoxyCodeLine{00383\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ num\_classes]\ representing\ the\ predicted\ logits\ }\textcolor{keywordflow}{for}\ each\ \textcolor{keyword}{class}}
\DoxyCodeLine{00384\ \ \ \ \ \ \ target\_tensor:\ A\ float\ tensor\ of\ shape\ [batch\_size,\ num\_anchors,}
\DoxyCodeLine{00385\ \ \ \ \ \ \ \ \ num\_classes]\ representing\ logit\ classification\ targets}
\DoxyCodeLine{00386\ \ \ \ \ \ \ weights:\ a\ float\ tensor\ of\ shape,\ either\ [batch\_size,\ num\_anchors,}
\DoxyCodeLine{00387\ \ \ \ \ \ \ \ \ num\_classes]\ \textcolor{keywordflow}{or}\ [batch\_size,\ num\_anchors,\ 1].\ If\ the\ shape\ \textcolor{keywordflow}{is}}
\DoxyCodeLine{00388\ \ \ \ \ \ \ \ \ [batch\_size,\ num\_anchors,\ 1],\ all\ the\ classses\ are\ equally\ weighted.}
\DoxyCodeLine{00389\ }
\DoxyCodeLine{00390\ \ \ \ \ Returns:}
\DoxyCodeLine{00391\ \ \ \ \ \ \ loss:\ a\ float\ tensor\ of\ shape\ [batch\_size,\ num\_anchors]}
\DoxyCodeLine{00392\ \ \ \ \ \ \ \ \ representing\ the\ value\ of\ the\ loss\ function.}
\DoxyCodeLine{00393\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}}}
\DoxyCodeLine{00394\ \textcolor{stringliteral}{\ \ \ \ weights\ =\ tf.reduce\_mean(weights,\ axis=2)}}
\DoxyCodeLine{00395\ \textcolor{stringliteral}{\ \ \ \ num\_classes\ =\ prediction\_tensor.get\_shape().as\_list()[-\/1]}}
\DoxyCodeLine{00396\ \textcolor{stringliteral}{\ \ \ \ target\_tensor\ =\ self.\_scale\_and\_softmax\_logits(target\_tensor)}}
\DoxyCodeLine{00397\ \textcolor{stringliteral}{\ \ \ \ prediction\_tensor\ =\ tf.divide(prediction\_tensor,\ self.\_logit\_scale,}}
\DoxyCodeLine{00398\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ name='scale\_logits'})}
\DoxyCodeLine{00399\ }
\DoxyCodeLine{00400\ \ \ \ \ per\_row\_cross\_ent\ =\ (tf.nn.softmax\_cross\_entropy\_with\_logits(}
\DoxyCodeLine{00401\ \ \ \ \ \ \ \ \ labels=tf.reshape(target\_tensor,\ [-\/1,\ num\_classes]),}
\DoxyCodeLine{00402\ \ \ \ \ \ \ \ \ logits=tf.reshape(prediction\_tensor,\ [-\/1,\ num\_classes])))}
\DoxyCodeLine{00403\ \ \ \ \ \textcolor{keywordflow}{return}\ tf.reshape(per\_row\_cross\_ent,\ tf.shape(weights))\ *\ weights}
\DoxyCodeLine{00404\ }
\DoxyCodeLine{00405\ }

\end{DoxyCode}
Here is the call graph for this function\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=350pt]{d5/d4c/classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss_a34c03e1dffb28a11c360679afedec8e4_cgraph}
\end{center}
\end{figure}
Here is the caller graph for this function\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=350pt]{d5/d4c/classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss_a34c03e1dffb28a11c360679afedec8e4_icgraph}
\end{center}
\end{figure}
\Hypertarget{classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss_a7fb6902c62d6a331ae8dd5521b6a1089}\label{classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss_a7fb6902c62d6a331ae8dd5521b6a1089} 
\index{detection\_utils.core.losses.WeightedSoftmaxClassificationAgainstLogitsLoss@{detection\_utils.core.losses.WeightedSoftmaxClassificationAgainstLogitsLoss}!\_scale\_and\_softmax\_logits@{\_scale\_and\_softmax\_logits}}
\index{\_scale\_and\_softmax\_logits@{\_scale\_and\_softmax\_logits}!detection\_utils.core.losses.WeightedSoftmaxClassificationAgainstLogitsLoss@{detection\_utils.core.losses.WeightedSoftmaxClassificationAgainstLogitsLoss}}
\doxysubsubsection{\texorpdfstring{\_scale\_and\_softmax\_logits()}{\_scale\_and\_softmax\_logits()}}
{\footnotesize\ttfamily detection\+\_\+utils.\+core.\+losses.\+Weighted\+Softmax\+Classification\+Against\+Logits\+Loss.\+\_\+scale\+\_\+and\+\_\+softmax\+\_\+logits (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{logits }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Scale logits then apply softmax.\end{DoxyVerb}
 

Definition at line \mbox{\hyperlink{losses_8py_source_l00373}{373}} of file \mbox{\hyperlink{losses_8py_source}{losses.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00373\ \ \ \textcolor{keyword}{def\ }\_scale\_and\_softmax\_logits(self,\ logits):}
\DoxyCodeLine{00374\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Scale\ logits\ then\ apply\ softmax."{}"{}"{}}}
\DoxyCodeLine{00375\ \ \ \ \ scaled\_logits\ =\ tf.divide(logits,\ self.\_logit\_scale,\ name=\textcolor{stringliteral}{'scale\_logits'})}
\DoxyCodeLine{00376\ \ \ \ \ \textcolor{keywordflow}{return}\ tf.nn.softmax(scaled\_logits,\ name=\textcolor{stringliteral}{'convert\_scores'})}
\DoxyCodeLine{00377\ }

\end{DoxyCode}
Here is the caller graph for this function\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=350pt]{d5/d4c/classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss_a7fb6902c62d6a331ae8dd5521b6a1089_icgraph}
\end{center}
\end{figure}


\doxysubsection{Member Data Documentation}
\Hypertarget{classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss_a90b59234980cf95e5139fc5e2f00843e}\label{classdetection__utils_1_1core_1_1losses_1_1_weighted_softmax_classification_against_logits_loss_a90b59234980cf95e5139fc5e2f00843e} 
\index{detection\_utils.core.losses.WeightedSoftmaxClassificationAgainstLogitsLoss@{detection\_utils.core.losses.WeightedSoftmaxClassificationAgainstLogitsLoss}!\_logit\_scale@{\_logit\_scale}}
\index{\_logit\_scale@{\_logit\_scale}!detection\_utils.core.losses.WeightedSoftmaxClassificationAgainstLogitsLoss@{detection\_utils.core.losses.WeightedSoftmaxClassificationAgainstLogitsLoss}}
\doxysubsubsection{\texorpdfstring{\_logit\_scale}{\_logit\_scale}}
{\footnotesize\ttfamily detection\+\_\+utils.\+core.\+losses.\+Weighted\+Softmax\+Classification\+Against\+Logits\+Loss.\+\_\+logit\+\_\+scale\hspace{0.3cm}{\ttfamily [protected]}}



Definition at line \mbox{\hyperlink{losses_8py_source_l00371}{371}} of file \mbox{\hyperlink{losses_8py_source}{losses.\+py}}.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
src/main/resources/processing/video/detections/detection\+\_\+utils/core/\mbox{\hyperlink{losses_8py}{losses.\+py}}\end{DoxyCompactItemize}
