<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.7"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>RESIIST: detection_utils.utils.learning_schedules Namespace Reference</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">RESIIST
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.7 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('../../',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="../../d8/d97/namespacedetection__utils.html">detection_utils</a></li><li class="navelem"><a class="el" href="../../d5/d2b/namespacedetection__utils_1_1utils.html">utils</a></li><li class="navelem"><a class="el" href="../../d5/d6e/namespacedetection__utils_1_1utils_1_1learning__schedules.html">learning_schedules</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle"><div class="title">detection_utils.utils.learning_schedules Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a58612d7cddcf21d6cde5bd725f4665ad"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d5/d6e/namespacedetection__utils_1_1utils_1_1learning__schedules.html#a58612d7cddcf21d6cde5bd725f4665ad">exponential_decay_with_burnin</a> (global_step, learning_rate_base, learning_rate_decay_steps, learning_rate_decay_factor, burnin_learning_rate=0.0, burnin_steps=0, min_learning_rate=0.0, staircase=True)</td></tr>
<tr class="separator:a58612d7cddcf21d6cde5bd725f4665ad"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abb26a1c6ade9443a3a23d2dda73ca031"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d5/d6e/namespacedetection__utils_1_1utils_1_1learning__schedules.html#abb26a1c6ade9443a3a23d2dda73ca031">cosine_decay_with_warmup</a> (global_step, learning_rate_base, total_steps, warmup_learning_rate=0.0, warmup_steps=0, hold_base_rate_steps=0)</td></tr>
<tr class="separator:abb26a1c6ade9443a3a23d2dda73ca031"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a58bc49f44c22512b9fc3ca5a179d24a5"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d5/d6e/namespacedetection__utils_1_1utils_1_1learning__schedules.html#a58bc49f44c22512b9fc3ca5a179d24a5">manual_stepping</a> (global_step, boundaries, rates, warmup=False)</td></tr>
<tr class="separator:a58bc49f44c22512b9fc3ca5a179d24a5"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">Library of common learning rate schedules.</pre> </div><h2 class="groupheader">Function Documentation</h2>
<a id="abb26a1c6ade9443a3a23d2dda73ca031" name="abb26a1c6ade9443a3a23d2dda73ca031"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abb26a1c6ade9443a3a23d2dda73ca031">&#9670;&#160;</a></span>cosine_decay_with_warmup()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">detection_utils.utils.learning_schedules.cosine_decay_with_warmup </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_step</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>learning_rate_base</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>total_steps</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>warmup_learning_rate</em> = <code>0.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>warmup_steps</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>hold_base_rate_steps</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Cosine decay schedule with warm up period.

Cosine annealing learning rate as described in:
  Loshchilov and Hutter, SGDR: Stochastic Gradient Descent with Warm Restarts.
  ICLR 2017. https://arxiv.org/abs/1608.03983
In this schedule, the learning rate grows linearly from warmup_learning_rate
to learning_rate_base for warmup_steps, then transitions to a cosine decay
schedule.

Args:
  global_step: int64 (scalar) tensor representing global step.
  learning_rate_base: base learning rate.
  total_steps: total number of training steps.
  warmup_learning_rate: initial learning rate for warm up.
  warmup_steps: number of warmup steps.
  hold_base_rate_steps: Optional number of steps to hold base learning rate
    before decaying.

Returns:
  If executing eagerly:
    returns a no-arg callable that outputs the (scalar)
    float tensor learning rate given the current value of global_step.
  If in a graph:
    immediately returns a (scalar) float tensor representing learning rate.

Raises:
  ValueError: if warmup_learning_rate is larger than learning_rate_base,
    or if warmup_steps is larger than total_steps.
</pre> 
<p class="definition">Definition at line <a class="el" href="../../d5/d34/learning__schedules_8py_source.html#l00085">85</a> of file <a class="el" href="../../d5/d34/learning__schedules_8py_source.html">learning_schedules.py</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">   90</span>                             hold_base_rate_steps=0):</div>
<div class="line"><span class="lineno">   91</span>  <span class="stringliteral">&quot;&quot;&quot;Cosine decay schedule with warm up period.</span></div>
<div class="line"><span class="lineno">   92</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   93</span><span class="stringliteral">  Cosine annealing learning rate </span><span class="keyword">as</span> described <span class="keywordflow">in</span>:</div>
<div class="line"><span class="lineno">   94</span>    Loshchilov <span class="keywordflow">and</span> Hutter, SGDR: Stochastic Gradient Descent <span class="keyword">with</span> Warm Restarts.</div>
<div class="line"><span class="lineno">   95</span>    ICLR 2017. https://arxiv.org/abs/1608.03983</div>
<div class="line"><span class="lineno">   96</span>  In this schedule, the learning rate grows linearly <span class="keyword">from</span> warmup_learning_rate</div>
<div class="line"><span class="lineno">   97</span>  to learning_rate_base <span class="keywordflow">for</span> warmup_steps, then transitions to a cosine decay</div>
<div class="line"><span class="lineno">   98</span>  schedule.</div>
<div class="line"><span class="lineno">   99</span> </div>
<div class="line"><span class="lineno">  100</span>  Args:</div>
<div class="line"><span class="lineno">  101</span>    global_step: int64 (scalar) tensor representing <span class="keyword">global</span> step.</div>
<div class="line"><span class="lineno">  102</span>    learning_rate_base: base learning rate.</div>
<div class="line"><span class="lineno">  103</span>    total_steps: total number of training steps.</div>
<div class="line"><span class="lineno">  104</span>    warmup_learning_rate: initial learning rate <span class="keywordflow">for</span> warm up.</div>
<div class="line"><span class="lineno">  105</span>    warmup_steps: number of warmup steps.</div>
<div class="line"><span class="lineno">  106</span>    hold_base_rate_steps: Optional number of steps to hold base learning rate</div>
<div class="line"><span class="lineno">  107</span>      before decaying.</div>
<div class="line"><span class="lineno">  108</span> </div>
<div class="line"><span class="lineno">  109</span>  Returns:</div>
<div class="line"><span class="lineno">  110</span>    If executing eagerly:</div>
<div class="line"><span class="lineno">  111</span>      returns a no-arg callable that outputs the (scalar)</div>
<div class="line"><span class="lineno">  112</span>      float tensor learning rate given the current value of global_step.</div>
<div class="line"><span class="lineno">  113</span>    If <span class="keywordflow">in</span> a graph:</div>
<div class="line"><span class="lineno">  114</span>      immediately returns a (scalar) float tensor representing learning rate.</div>
<div class="line"><span class="lineno">  115</span> </div>
<div class="line"><span class="lineno">  116</span>  Raises:</div>
<div class="line"><span class="lineno">  117</span>    ValueError: <span class="keywordflow">if</span> warmup_learning_rate <span class="keywordflow">is</span> larger than learning_rate_base,</div>
<div class="line"><span class="lineno">  118</span>      <span class="keywordflow">or</span> <span class="keywordflow">if</span> warmup_steps <span class="keywordflow">is</span> larger than total_steps.</div>
<div class="line"><span class="lineno">  119</span>  <span class="stringliteral">&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  120</span><span class="stringliteral">  </span><span class="keywordflow">if</span> total_steps &lt; warmup_steps:</div>
<div class="line"><span class="lineno">  121</span>    <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&#39;total_steps must be larger or equal to &#39;</span></div>
<div class="line"><span class="lineno">  122</span>                     <span class="stringliteral">&#39;warmup_steps.&#39;</span>)</div>
<div class="line"><span class="lineno">  123</span>  <span class="keyword">def </span>eager_decay_rate():</div>
<div class="line"><span class="lineno">  124</span>    <span class="stringliteral">&quot;&quot;&quot;Callable to compute the learning rate.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  125</span>    learning_rate = 0.5 * learning_rate_base * (1 + tf.cos(</div>
<div class="line"><span class="lineno">  126</span>        np.pi *</div>
<div class="line"><span class="lineno">  127</span>        (tf.cast(global_step, tf.float32) - warmup_steps - hold_base_rate_steps</div>
<div class="line"><span class="lineno">  128</span>        ) / float(total_steps - warmup_steps - hold_base_rate_steps)))</div>
<div class="line"><span class="lineno">  129</span>    <span class="keywordflow">if</span> hold_base_rate_steps &gt; 0:</div>
<div class="line"><span class="lineno">  130</span>      learning_rate = tf.where(</div>
<div class="line"><span class="lineno">  131</span>          global_step &gt; warmup_steps + hold_base_rate_steps,</div>
<div class="line"><span class="lineno">  132</span>          learning_rate, learning_rate_base)</div>
<div class="line"><span class="lineno">  133</span>    <span class="keywordflow">if</span> warmup_steps &gt; 0:</div>
<div class="line"><span class="lineno">  134</span>      <span class="keywordflow">if</span> learning_rate_base &lt; warmup_learning_rate:</div>
<div class="line"><span class="lineno">  135</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&#39;learning_rate_base must be larger or equal to &#39;</span></div>
<div class="line"><span class="lineno">  136</span>                         <span class="stringliteral">&#39;warmup_learning_rate.&#39;</span>)</div>
<div class="line"><span class="lineno">  137</span>      slope = (learning_rate_base - warmup_learning_rate) / warmup_steps</div>
<div class="line"><span class="lineno">  138</span>      warmup_rate = slope * tf.cast(global_step,</div>
<div class="line"><span class="lineno">  139</span>                                    tf.float32) + warmup_learning_rate</div>
<div class="line"><span class="lineno">  140</span>      learning_rate = tf.where(global_step &lt; warmup_steps, warmup_rate,</div>
<div class="line"><span class="lineno">  141</span>                               learning_rate)</div>
<div class="line"><span class="lineno">  142</span>    <span class="keywordflow">return</span> tf.where(global_step &gt; total_steps, 0.0, learning_rate,</div>
<div class="line"><span class="lineno">  143</span>                    name=<span class="stringliteral">&#39;learning_rate&#39;</span>)</div>
<div class="line"><span class="lineno">  144</span> </div>
<div class="line"><span class="lineno">  145</span>  <span class="keywordflow">if</span> tf.executing_eagerly():</div>
<div class="line"><span class="lineno">  146</span>    <span class="keywordflow">return</span> eager_decay_rate</div>
<div class="line"><span class="lineno">  147</span>  <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  148</span>    <span class="keywordflow">return</span> eager_decay_rate()</div>
<div class="line"><span class="lineno">  149</span> </div>
<div class="line"><span class="lineno">  150</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a58612d7cddcf21d6cde5bd725f4665ad" name="a58612d7cddcf21d6cde5bd725f4665ad"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a58612d7cddcf21d6cde5bd725f4665ad">&#9670;&#160;</a></span>exponential_decay_with_burnin()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">detection_utils.utils.learning_schedules.exponential_decay_with_burnin </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_step</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>learning_rate_base</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>learning_rate_decay_steps</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>learning_rate_decay_factor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>burnin_learning_rate</em> = <code>0.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>burnin_steps</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>min_learning_rate</em> = <code>0.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>staircase</em> = <code>True</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Exponential decay schedule with burn-in period.

In this schedule, learning rate is fixed at burnin_learning_rate
for a fixed period, before transitioning to a regular exponential
decay schedule.

Args:
  global_step: int tensor representing global step.
  learning_rate_base: base learning rate.
  learning_rate_decay_steps: steps to take between decaying the learning rate.
    Note that this includes the number of burn-in steps.
  learning_rate_decay_factor: multiplicative factor by which to decay
    learning rate.
  burnin_learning_rate: initial learning rate during burn-in period.  If
    0.0 (which is the default), then the burn-in learning rate is simply
    set to learning_rate_base.
  burnin_steps: number of steps to use burnin learning rate.
  min_learning_rate: the minimum learning rate.
  staircase: whether use staircase decay.

Returns:
  If executing eagerly:
    returns a no-arg callable that outputs the (scalar)
    float tensor learning rate given the current value of global_step.
  If in a graph:
    immediately returns a (scalar) float tensor representing learning rate.
</pre> 
<p class="definition">Definition at line <a class="el" href="../../d5/d34/learning__schedules_8py_source.html#l00026">26</a> of file <a class="el" href="../../d5/d34/learning__schedules_8py_source.html">learning_schedules.py</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">   33</span>                                  staircase=<span class="keyword">True</span>):</div>
<div class="line"><span class="lineno">   34</span>  <span class="stringliteral">&quot;&quot;&quot;Exponential decay schedule with burn-in period.</span></div>
<div class="line"><span class="lineno">   35</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   36</span><span class="stringliteral">  In this schedule, learning rate </span><span class="keywordflow">is</span> fixed at burnin_learning_rate</div>
<div class="line"><span class="lineno">   37</span>  <span class="keywordflow">for</span> a fixed period, before transitioning to a regular exponential</div>
<div class="line"><span class="lineno">   38</span>  decay schedule.</div>
<div class="line"><span class="lineno">   39</span> </div>
<div class="line"><span class="lineno">   40</span>  Args:</div>
<div class="line"><span class="lineno">   41</span>    global_step: int tensor representing <span class="keyword">global</span> step.</div>
<div class="line"><span class="lineno">   42</span>    learning_rate_base: base learning rate.</div>
<div class="line"><span class="lineno">   43</span>    learning_rate_decay_steps: steps to take between decaying the learning rate.</div>
<div class="line"><span class="lineno">   44</span>      Note that this includes the number of burn-<span class="keywordflow">in</span> steps.</div>
<div class="line"><span class="lineno">   45</span>    learning_rate_decay_factor: multiplicative factor by which to decay</div>
<div class="line"><span class="lineno">   46</span>      learning rate.</div>
<div class="line"><span class="lineno">   47</span>    burnin_learning_rate: initial learning rate during burn-<span class="keywordflow">in</span> period.  If</div>
<div class="line"><span class="lineno">   48</span>      0.0 (which <span class="keywordflow">is</span> the default), then the burn-<span class="keywordflow">in</span> learning rate <span class="keywordflow">is</span> simply</div>
<div class="line"><span class="lineno">   49</span>      set to learning_rate_base.</div>
<div class="line"><span class="lineno">   50</span>    burnin_steps: number of steps to use burnin learning rate.</div>
<div class="line"><span class="lineno">   51</span>    min_learning_rate: the minimum learning rate.</div>
<div class="line"><span class="lineno">   52</span>    staircase: whether use staircase decay.</div>
<div class="line"><span class="lineno">   53</span> </div>
<div class="line"><span class="lineno">   54</span>  Returns:</div>
<div class="line"><span class="lineno">   55</span>    If executing eagerly:</div>
<div class="line"><span class="lineno">   56</span>      returns a no-arg callable that outputs the (scalar)</div>
<div class="line"><span class="lineno">   57</span>      float tensor learning rate given the current value of global_step.</div>
<div class="line"><span class="lineno">   58</span>    If <span class="keywordflow">in</span> a graph:</div>
<div class="line"><span class="lineno">   59</span>      immediately returns a (scalar) float tensor representing learning rate.</div>
<div class="line"><span class="lineno">   60</span>  <span class="stringliteral">&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">   61</span><span class="stringliteral">  </span><span class="keywordflow">if</span> burnin_learning_rate == 0:</div>
<div class="line"><span class="lineno">   62</span>    burnin_learning_rate = learning_rate_base</div>
<div class="line"><span class="lineno">   63</span> </div>
<div class="line"><span class="lineno">   64</span>  <span class="keyword">def </span>eager_decay_rate():</div>
<div class="line"><span class="lineno">   65</span>    <span class="stringliteral">&quot;&quot;&quot;Callable to compute the learning rate.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">   66</span>    post_burnin_learning_rate = tf.train.exponential_decay(</div>
<div class="line"><span class="lineno">   67</span>        learning_rate_base,</div>
<div class="line"><span class="lineno">   68</span>        global_step - burnin_steps,</div>
<div class="line"><span class="lineno">   69</span>        learning_rate_decay_steps,</div>
<div class="line"><span class="lineno">   70</span>        learning_rate_decay_factor,</div>
<div class="line"><span class="lineno">   71</span>        staircase=staircase)</div>
<div class="line"><span class="lineno">   72</span>    <span class="keywordflow">if</span> callable(post_burnin_learning_rate):</div>
<div class="line"><span class="lineno">   73</span>      post_burnin_learning_rate = post_burnin_learning_rate()</div>
<div class="line"><span class="lineno">   74</span>    <span class="keywordflow">return</span> tf.maximum(tf.where(</div>
<div class="line"><span class="lineno">   75</span>        tf.less(tf.cast(global_step, tf.int32), tf.constant(burnin_steps)),</div>
<div class="line"><span class="lineno">   76</span>        tf.constant(burnin_learning_rate),</div>
<div class="line"><span class="lineno">   77</span>        post_burnin_learning_rate), min_learning_rate, name=<span class="stringliteral">&#39;learning_rate&#39;</span>)</div>
<div class="line"><span class="lineno">   78</span> </div>
<div class="line"><span class="lineno">   79</span>  <span class="keywordflow">if</span> tf.executing_eagerly():</div>
<div class="line"><span class="lineno">   80</span>    <span class="keywordflow">return</span> eager_decay_rate</div>
<div class="line"><span class="lineno">   81</span>  <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">   82</span>    <span class="keywordflow">return</span> eager_decay_rate()</div>
<div class="line"><span class="lineno">   83</span> </div>
<div class="line"><span class="lineno">   84</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a58bc49f44c22512b9fc3ca5a179d24a5" name="a58bc49f44c22512b9fc3ca5a179d24a5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a58bc49f44c22512b9fc3ca5a179d24a5">&#9670;&#160;</a></span>manual_stepping()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">detection_utils.utils.learning_schedules.manual_stepping </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_step</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>boundaries</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>rates</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>warmup</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Manually stepped learning rate schedule.

This function provides fine grained control over learning rates.  One must
specify a sequence of learning rates as well as a set of integer steps
at which the current learning rate must transition to the next.  For example,
if boundaries = [5, 10] and rates = [.1, .01, .001], then the learning
rate returned by this function is .1 for global_step=0,...,4, .01 for
global_step=5...9, and .001 for global_step=10 and onward.

Args:
  global_step: int64 (scalar) tensor representing global step.
  boundaries: a list of global steps at which to switch learning
    rates.  This list is assumed to consist of increasing positive integers.
  rates: a list of (float) learning rates corresponding to intervals between
    the boundaries.  The length of this list must be exactly
    len(boundaries) + 1.
  warmup: Whether to linearly interpolate learning rate for steps in
    [0, boundaries[0]].

Returns:
  If executing eagerly:
    returns a no-arg callable that outputs the (scalar)
    float tensor learning rate given the current value of global_step.
  If in a graph:
    immediately returns a (scalar) float tensor representing learning rate.
Raises:
  ValueError: if one of the following checks fails:
    1. boundaries is a strictly increasing list of positive integers
    2. len(rates) == len(boundaries) + 1
    3. boundaries[0] != 0
</pre> 
<p class="definition">Definition at line <a class="el" href="../../d5/d34/learning__schedules_8py_source.html#l00151">151</a> of file <a class="el" href="../../d5/d34/learning__schedules_8py_source.html">learning_schedules.py</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  151</span><span class="keyword">def </span>manual_stepping(global_step, boundaries, rates, warmup=False):</div>
<div class="line"><span class="lineno">  152</span>  <span class="stringliteral">&quot;&quot;&quot;Manually stepped learning rate schedule.</span></div>
<div class="line"><span class="lineno">  153</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  154</span><span class="stringliteral">  This function provides fine grained control over learning rates.  One must</span></div>
<div class="line"><span class="lineno">  155</span><span class="stringliteral">  specify a sequence of learning rates </span><span class="keyword">as</span> well <span class="keyword">as</span> a set of integer steps</div>
<div class="line"><span class="lineno">  156</span>  at which the current learning rate must transition to the next.  For example,</div>
<div class="line"><span class="lineno">  157</span>  <span class="keywordflow">if</span> boundaries = [5, 10] <span class="keywordflow">and</span> rates = [.1, .01, .001], then the learning</div>
<div class="line"><span class="lineno">  158</span>  rate returned by this function <span class="keywordflow">is</span> .1 <span class="keywordflow">for</span> global_step=0,...,4, .01 <span class="keywordflow">for</span></div>
<div class="line"><span class="lineno">  159</span>  global_step=5...9, <span class="keywordflow">and</span> .001 <span class="keywordflow">for</span> global_step=10 <span class="keywordflow">and</span> onward.</div>
<div class="line"><span class="lineno">  160</span> </div>
<div class="line"><span class="lineno">  161</span>  Args:</div>
<div class="line"><span class="lineno">  162</span>    global_step: int64 (scalar) tensor representing <span class="keyword">global</span> step.</div>
<div class="line"><span class="lineno">  163</span>    boundaries: a list of <span class="keyword">global</span> steps at which to switch learning</div>
<div class="line"><span class="lineno">  164</span>      rates.  This list <span class="keywordflow">is</span> assumed to consist of increasing positive integers.</div>
<div class="line"><span class="lineno">  165</span>    rates: a list of (float) learning rates corresponding to intervals between</div>
<div class="line"><span class="lineno">  166</span>      the boundaries.  The length of this list must be exactly</div>
<div class="line"><span class="lineno">  167</span>      len(boundaries) + 1.</div>
<div class="line"><span class="lineno">  168</span>    warmup: Whether to linearly interpolate learning rate <span class="keywordflow">for</span> steps <span class="keywordflow">in</span></div>
<div class="line"><span class="lineno">  169</span>      [0, boundaries[0]].</div>
<div class="line"><span class="lineno">  170</span> </div>
<div class="line"><span class="lineno">  171</span>  Returns:</div>
<div class="line"><span class="lineno">  172</span>    If executing eagerly:</div>
<div class="line"><span class="lineno">  173</span>      returns a no-arg callable that outputs the (scalar)</div>
<div class="line"><span class="lineno">  174</span>      float tensor learning rate given the current value of global_step.</div>
<div class="line"><span class="lineno">  175</span>    If <span class="keywordflow">in</span> a graph:</div>
<div class="line"><span class="lineno">  176</span>      immediately returns a (scalar) float tensor representing learning rate.</div>
<div class="line"><span class="lineno">  177</span>  Raises:</div>
<div class="line"><span class="lineno">  178</span>    ValueError: <span class="keywordflow">if</span> one of the following checks fails:</div>
<div class="line"><span class="lineno">  179</span>      1. boundaries <span class="keywordflow">is</span> a strictly increasing list of positive integers</div>
<div class="line"><span class="lineno">  180</span>      2. len(rates) == len(boundaries) + 1</div>
<div class="line"><span class="lineno">  181</span>      3. boundaries[0] != 0</div>
<div class="line"><span class="lineno">  182</span>  <span class="stringliteral">&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  183</span><span class="stringliteral">  </span><span class="keywordflow">if</span> any([b &lt; 0 <span class="keywordflow">for</span> b <span class="keywordflow">in</span> boundaries]) <span class="keywordflow">or</span> any(</div>
<div class="line"><span class="lineno">  184</span>      [<span class="keywordflow">not</span> isinstance(b, int) <span class="keywordflow">for</span> b <span class="keywordflow">in</span> boundaries]):</div>
<div class="line"><span class="lineno">  185</span>    <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&#39;boundaries must be a list of positive integers&#39;</span>)</div>
<div class="line"><span class="lineno">  186</span>  <span class="keywordflow">if</span> any([bnext &lt;= b <span class="keywordflow">for</span> bnext, b <span class="keywordflow">in</span> zip(boundaries[1:], boundaries[:-1])]):</div>
<div class="line"><span class="lineno">  187</span>    <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&#39;Entries in boundaries must be strictly increasing.&#39;</span>)</div>
<div class="line"><span class="lineno">  188</span>  <span class="keywordflow">if</span> any([<span class="keywordflow">not</span> isinstance(r, float) <span class="keywordflow">for</span> r <span class="keywordflow">in</span> rates]):</div>
<div class="line"><span class="lineno">  189</span>    <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&#39;Learning rates must be floats&#39;</span>)</div>
<div class="line"><span class="lineno">  190</span>  <span class="keywordflow">if</span> len(rates) != len(boundaries) + 1:</div>
<div class="line"><span class="lineno">  191</span>    <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&#39;Number of provided learning rates must exceed &#39;</span></div>
<div class="line"><span class="lineno">  192</span>                     <span class="stringliteral">&#39;number of boundary points by exactly 1.&#39;</span>)</div>
<div class="line"><span class="lineno">  193</span> </div>
<div class="line"><span class="lineno">  194</span>  <span class="keywordflow">if</span> boundaries <span class="keywordflow">and</span> boundaries[0] == 0:</div>
<div class="line"><span class="lineno">  195</span>    <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&#39;First step cannot be zero.&#39;</span>)</div>
<div class="line"><span class="lineno">  196</span> </div>
<div class="line"><span class="lineno">  197</span>  <span class="keywordflow">if</span> warmup <span class="keywordflow">and</span> boundaries:</div>
<div class="line"><span class="lineno">  198</span>    slope = (rates[1] - rates[0]) * 1.0 / boundaries[0]</div>
<div class="line"><span class="lineno">  199</span>    warmup_steps = list(range(boundaries[0]))</div>
<div class="line"><span class="lineno">  200</span>    warmup_rates = [rates[0] + slope * step <span class="keywordflow">for</span> step <span class="keywordflow">in</span> warmup_steps]</div>
<div class="line"><span class="lineno">  201</span>    boundaries = warmup_steps + boundaries</div>
<div class="line"><span class="lineno">  202</span>    rates = warmup_rates + rates[1:]</div>
<div class="line"><span class="lineno">  203</span>  <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  204</span>    boundaries = [0] + boundaries</div>
<div class="line"><span class="lineno">  205</span>  num_boundaries = len(boundaries)</div>
<div class="line"><span class="lineno">  206</span> </div>
<div class="line"><span class="lineno">  207</span>  <span class="keyword">def </span>eager_decay_rate():</div>
<div class="line"><span class="lineno">  208</span>    <span class="stringliteral">&quot;&quot;&quot;Callable to compute the learning rate.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  209</span>    rate_index = tf.reduce_max(tf.where(</div>
<div class="line"><span class="lineno">  210</span>        tf.greater_equal(global_step, boundaries),</div>
<div class="line"><span class="lineno">  211</span>        list(range(num_boundaries)),</div>
<div class="line"><span class="lineno">  212</span>        [0] * num_boundaries))</div>
<div class="line"><span class="lineno">  213</span>    <span class="keywordflow">return</span> tf.reduce_sum(rates * tf.one_hot(rate_index, depth=num_boundaries),</div>
<div class="line"><span class="lineno">  214</span>                         name=<span class="stringliteral">&#39;learning_rate&#39;</span>)</div>
<div class="line"><span class="lineno">  215</span>  <span class="keywordflow">if</span> tf.executing_eagerly():</div>
<div class="line"><span class="lineno">  216</span>    <span class="keywordflow">return</span> eager_decay_rate</div>
<div class="line"><span class="lineno">  217</span>  <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  218</span>    <span class="keywordflow">return</span> eager_decay_rate()</div>
</div><!-- fragment -->
</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="../../doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.7
</small></address>
</body>
</html>
